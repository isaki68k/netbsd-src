<!-- Creator     : groff version 1.19.2 -->
<!-- CreationDate: Fri Mar  2 13:29:19 2018 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="Content-Style" content="text/css">
<style type="text/css">
p.p1 {
	margin-left: 8%;
}
pre.p1 {
	margin-top: 1em;
	margin-left: 12%;
	margin-bottom: 1em;
}
dt.p1 {
	margin-top: 1em;
	margin-left: 8%;
	font-family: monospace;
}
dd.p1 {
	margin-left: 12%;
}
table.p1 {
	margin-left: 8%;
}

SPAN.plain {
	color: black;
}

SPAN.n7 {
	background-color: rgb(255,255,153);	/* クリーム */
}

P.n7 {
	background-color: rgb(255,255,153);	/* クリーム */
}
SPAN.del {
	background-color: rgb(255,209,209);	/* ピンク */
}
SPAN.bad {
	color: rgb(255,40,0);	/* 赤 */
	font-weight: bold;
}
SPAN.comment {
	color: rgb(53,161,107);	/* 緑 */
	font-weight: bold;
}
SPAN.a2 {
	color: rgb(0,65,255);	/* 青 */
	font-weight: bold;
}
DEL.a2 {
	text-decoration: underline;
	color: rgb(0,65,255);	/* 青 */
}
</style>
<title></title>
</head>
<body>
<p>
凡例:<br>
<span class=a2 style="margin-left:8%;">青太字は AUDIO2 での仕様(案)。</span><br>
<span class=del style="margin-left:8%">赤背景は AUDIO2 で削除したいところ(案)。</span><br>
<span class=comment style="margin-left:8%;">緑太字はその他のコメント</span>

<hr>


<b>NAME</b>

<p class=p1><b>audio</b> &mdash;
interface between low and high level audio drivers
</p>

<b>DESCRIPTION</b>

<p class=p1>The audio device driver is divided into a
high level, hardware independent layer, and a low level
hardware dependent layer. The interface between these is the
structure.</p>

<pre class=p1>
struct audio_hw_if {
     int     (*open)(void *, int);
     void    (*close)(void *);
     <span class=del>int     (*drain)(void *);</span>

     <span class=del>int     (*query_encoding)(void *, struct audio_encoding *);</span>
     <span class=del>int     (*set_params)(void *, int, int,</span>
		 <span class=del>audio_params_t *, audio_params_t *,</span>
		 <span class=del>stream_filter_list_t *, stream_filter_list_t *);</span>
     <span class=a2>int     (*query_format)(void *, audio_format_query_t *);</span>
     <span class=a2>int     (*set_format)(void *, int,
		 const audio_params_t *, const audio_params_t *,
		 audio_filter_reg_t *, audio_filter_reg_t *);</span>
     int     (*round_blocksize)(void *, int, int, const audio_params_t *);

     int     (*commit_settings)(void *);

     int     (*init_output)(void *, void *, int);
     int     (*init_input)(void *, void *, int);
     int     (*start_output)(void *, void *, int, void (*)(void *), void *);
     int     (*start_input)(void *, void *, int, void (*)(void *), void *);
     int     (*halt_output)(void *);
     int     (*halt_input)(void *);

     int     (*speaker_ctl)(void *, int);
#define SPKR_ON  1
#define SPKR_OFF 0

     int     (*getdev)(void *, struct audio_device *);
     <span class=del>int     (*setfd)(void *, int);</span>

     int     (*set_port)(void *, mixer_ctrl_t *);
     int     (*get_port)(void *, mixer_ctrl_t *);

     int     (*query_devinfo)(void *, mixer_devinfo_t *);

     void    *(*allocm)(void *, int, size_t);
     void    (*freem)(void *, void *, size_t);
     size_t  (*round_buffersize)(void *, int, size_t);
     <span class=del>paddr_t (*mappage)(void *, void *, off_t, int);</span>

     int     (*get_props)(void *);

     int     (*trigger_output)(void *, void *, void *, int,
		 void (*)(void *), void *, const audio_params_t *);
     int     (*trigger_input)(void *, void *, void *, int,
		 void (*)(void *), void *, const audio_params_t *);
     int     (*dev_ioctl)(void *, u_long, void *, int, struct lwp *);
     void    (*get_locks)(void *, kmutex_t **, kmutex_t **);
};

typedef struct audio_params {
     u_int   sample_rate;    /* sample rate */
     u_int   encoding;	     /* e.g. mu-law, linear, etc */
     u_int   precision;	     /* bits/subframe */
     u_int   validbits;	     /* valid bits in a subframe */
     u_int   channels;	     /* mono(1), stereo(2) */
} audio_params_t;
</pre>

<p class=p1>
The high level audio driver attaches to the low level driver when the
latter calls audio_attach_mi.  This call should be</p>

<pre class=p1 style="margin-bottom: 0">
device_t
audio_attach_mi(const struct audio_hw_if *ahwp, void *hdl, device_t dev);
</pre>

<p class=p1>
     The <tt>audio_hw_if</tt> struct is as shown above.
	The <tt>hdl</tt> argument is a handle
     to some low level data structure.	It is sent as the first argument to
     all the functions in <tt>audio_hw_if</tt> when the high level driver calls them.
     <tt>dev</tt> is the device struct for the hardware device.
</p>

<p class=p1>
     The upper layer of the audio driver allocates one buffer for playing and
     one for recording.	 It handles the buffering of data from the user
     processes in these.  The data is presented to the lower level in smaller
     chunks, called blocks.  If, during playback, there is no data available
     from the user process when the hardware request another block a block of
     silence will be used instead.  Furthermore, if the user process does not
     read data quickly enough during recording data will be thrown away.
</p>

<p class=p1><span class=a2>
	The phase these functions are called is classified into three.
	Attach phase, Closed phase and Opened phase.
	Attach phase is during device attach and
	it transits to the Closed phase when the attach succeeded.
	Closed phase is when no sampling device is opened and
	it transits to the Opened phase when open succeeded.
	Opened phase is when any sampling device is opened and
	it transits to the Closed phase when close succeeded.
</p>

<p class=p1>
     The fields of <tt>audio_hw_if</tt> are described in some more detail below.	 Some
     fields are optional and can be set to NULL if not needed.
</p>

<dl>
<dt class=p1>int open(void *hdl, int flags)
<dd class=p1>
	     optional,
		<span class=del>is called when the audio device is opened.</span>
		<span class=a2>is called when the first device combining playback
		and recording is opened.</span>
		<span class=del>
		It should initialize the hardware for I/O.</span>
		<span class=a2>
		On a full duplex hardware,
		(FREAD | FWRITE) is passed to <i>flags</i>.
		On a half duplex hardware,
		FWRITE is passed for playback, or FREAD for recording.
		</span>
		Every successful call to open
	     is matched by a call to close.  Return 0 on success, otherwise an
	     error code.
		<span class=a2>It is called at Closed phase.</span>
</dd>

<dt class=p1>void close(void *hdl)
</dt>
<dd class=p1>
	     optional,
		<span class=del>is called when the audio device is closed.</span>
		<span class=a2>is called when the last audio device combining
		playback and recording is closed.</span>
		<span class=a2>It is called at Opened phase.</span>
</dd>

<dt class=p1><span class=del>int drain(void *hdl)</span>
</dt>
<dd class=p1><span class=del>
	     optional, is called before the device is closed or when
	     <tt>AUDIO_DRAIN</tt> is called.  It should make sure that no samples
	     remain in to be played that could be lost when close is called.
	     Return 0 on success, otherwise an error code.
</span></dd>
<dd class=p1><span class=comment>
The call on NetBSD8 makes no sense.
</span>
</dd>

<dt class=p1><span class=del>int query_encoding(void *hdl, struct audio_encoding *ae)</span>
</dt>
<dd class=p1><span class=del>
	     is used when <tt>AUDIO_GETENC</tt> is called.  It should fill the
	     audio_encoding structure and return 0 or, if there is no encoding
	     with the given number, return <tt>EINVAL</tt>.
</span></dd>
<dd class=p1><span class=comment>
It never called on NetBSD8.
</span></dd>

<dt class=p1><span class=a2>int query_format(void *hdl, audio_format_query_t *afp)
<dd class=p1><span class=a2>
	is called to enumerate formats supported by the hardware.
	It should fill the audio_format_t structure according to
	given number afp->index.
	If there is no format with given number, return <tt>EINVAL</tt>.
	It is called at any time.
<pre style="margin-left: -8%">
		typedef struct audio_format_query {
			u_int	index;
			struct audio_format fmt;
		} audio_format_query_t;
</pre>
	It is also used to determine the default format.
	The upper layer chooses the most preferred one as default format
	by following:<br>
	1. Higher priority is preferred (normally 0, the highest is 3, the lowest is 0).<br>
	2. AUDIO_ENCODING_SLINEAR_NE:16 is preferred if exists.<br>
	3. AUDIO_ENCODING_SLINEAR_OE:16 is preferred if exists.<br>
	4. More channels is preferred.<br>
</span>
<br>
<span class=a2>
	If the driver supports SLINEAR_NE:16 and the upper layer chooses it,
	the driver does not need to provide conversion function in set_format.
	Similarly, if the driver supports SLINEAR_OE:16 and the upper layer
	chooses it, the driver does not need to provide conversion function.
	Because the upper layer only supports conversion between SLINEAR_NE:16 and
	SLINEAR_OE:16 for convenience.
	If the upper layer chooses other format,
	the driver needs to provide conversion function in set_format.

	See also set_format.
	If the driver can not provide the conversion from/to SLINEAR_NE:16,
	set priority to -1.
	It means that the hardware supports this format but
	the driver does not (e.g. AC3), and it will never be choosen.
	</span>
	<p></p>
	<span class=comment>
	query_format が扱う audio_format_t はハードウェアが対応できる
	周波数が(離散値であっても帯域であっても)表現できる上、
	kent-audio2 以降に実装されたあるいはそれに追従したデバイスであれば
	すでに audio_format_t の配列を持っている可能性が高いので、
	運がよければそのまま使える。
	</span>
	<p></p>
	<span class=comment>
	今は query_format が定義されてなければ
	N8 同様 set_params をひたすら呼び出す互換動作を行いますが、
	たぶんこれは過渡期における一時的な救済措置です。
	<br>
</dd>
</span></dd>

<dt class=p1><span class=del>int set_params(void *hdl, int setmode, int usemode,
	     audio_params_t *play, audio_params_t *rec,
	     stream_filter_list_t *pfil, stream_filter_list_t *rfil)
</span></dt>
<dd class=p1><span class=del>
	     Called to set the audio encoding mode.  setmode is a combination
	     of the <tt>AUMODE_RECORD</tt> and <tt>AUMODE_PLAY</tt> flags to indicate which
	     mode(s) are to be set.  usemode is also a combination of these
	     flags, but indicates the current mode of the device (i.e., the
	     value of mode in the audio_info struct).
<br><br>
	     The play and rec structures contain the encoding parameters that
	     should be set.  The values of the structures may also be modified
	     if the hardware cannot be set to exactly the requested mode
	     (e.g., if the requested sampling rate is not supported, but one
	     close enough is).
<br><br>
	     If the hardware requires software assistance with some encoding
	     (e.g., it might be lacking mu-law support) it should fill the
	     pfil for playing or rfil for recording with conversion
	     information.  For example, if play requests [8000Hz, mu-law,
	     8/8bit, 1ch] and the hardware does not support 8bit mu-law, but
	     16bit slinear_le, the driver should call <tt>pfil-&gt;append()</tt> with
	     pfil, mulaw_to_linear16, and audio_params_t representing [8000Hz,
	     slinear_le, 16/16bit, 2ch].  If the driver needs multiple
	     conversions, a conversion nearest to the hardware should be set
	     to the head of pfil or rfil.  The definition of
	     stream_filter_list_t follows:

<pre style="margin-left: -8%; background-color: rgb(255,209,209);">
	     typedef struct stream_filter_list {
		     void (*append)(struct stream_filter_list *,
				    stream_filter_factory_t,
				    const audio_params_t *);
		     void (*prepend)(struct stream_filter_list *,
				     stream_filter_factory_t,
				     const audio_params_t *);
		     void (*set)(struct stream_filter_list *, int,
				 stream_filter_factory_t,
				 const audio_params_t *);
		     int req_size;
		     struct stream_filter_req {
			     stream_filter_factory_t *factory;
			     audio_params_t param; /* from-param for recording,
						      to-param for playing */
		     } filters[AUDIO_MAX_FILTERS];
	     } stream_filter_list_t;
</pre>
	     For playing, pfil constructs conversions as follows:
<pre style="margin-left: -8%; background-color: rgb(255,209,209);">
		     (play) == write(2) input
		       |     pfil->filters[pfil->req_size-1].factory
		     (pfil->filters[pfil->req_size-1].param)
		       |     pfil->filters[pfil->req_size-2].factory
		       :
		       |     pfil->filters[1].factory
		     (pfil->filters[1].param)
		       |     pfil->filters[0].factory
		     (pfil->filters[0].param)  == hardware input
</pre>
	     For recording, rfil constructs conversions as follows:
<pre style="margin-left: -8%; background-color: rgb(255,209,209);">
		     (rfil->filters[0].param) == hardware output
		       |     rfil->filters[0].factory
		     (rfil->filters[1].param)
		       |     rfil->filters[1].factory
		       :
		       |     rfil->filters[rfil->req_size-2].factory
		     (rfil->filters[rfil->req_size-1].param)
		       |     rfil->filters[rfil->req_size-1].factory
		     (rec)  == read(2) output
</pre>
	     If the device does not have the <tt>AUDIO_PROP_INDEPENDENT</tt> property
	     the same value is passed in both play and rec and the encoding
	     parameters from play is copied into rec after the call to
	     set_params.  Return 0 on success, otherwise an error code.
</span></dd>
<dd class=p1><span class=comment>
AUDIO2 ではこれを廃止する。代わりに set_format を新設。
</span></dd>

<dt class=p1><span class=a2>int set_foramt(void *hdl,
	int setmode,
	const audio_params_t *play, const audio_params_t *rec,
	audio_filter_reg_t *pfil, audio_filter_reg_t *rfil)
<dd class=p1><span class=a2>
	is called to set specified format to the hardware,
	when the device is attached or the hardware format is changed.
	<i>setmode</i> is a combination of the
	<tt>AUMODE_RECORD</tt> and <tt>AUMODE_PLAY</tt> flags to indicate which
	     modes are to be set.
<br><br>
	     The <i>play</i> and <i>rec</i> structures contain
	the encoding parameters that should be set to the hardware.
	If the driver has query_format interface, all parameters on
	<i>play</i> and/or <i>rec</i> are
	chosen from formats returned by query_format.
	Therefore <i>play</i> and/or <i>rec</i> are always settable.
	If the driver does not have query_format interface,
	the driver has to validate the format.
</span>
<br><br>
<span class=comment>
	set_params と違って選択したパラメータを play, rec に書き戻すことは
	出来ません。というか必要ないはずです。
</span>
<br><br>
<span class=a2>
	     If the hardware does not support
		<tt>AUDIO_ENCODING_SLINEAR_{NE,OE}:16</tt>,
	     conversion information should be filled the
	     pfil for playing or rfil for recording.
	The definition of audio_filter_reg_t and related structure follow:
<br><br>
<pre style="margin-left: 8%">
typedef struct {
	const void *src;
	const audio_format2_t *srcfmt;
	void *dst;
	const audio_format2_t *dstfmt;
	int count;
	void *context;
} audio_filter_arg_t;

typedef void(*audio_filter_t)(audio_filter_arg_t *arg);

typedef struct {
	audio_filter_t codec;
	void *context;
} audio_filter_reg_t;
</pre>
	<i>codec</i> is conversion function and
	<i>context</i> is optional opaque pointer passed to <i>codec</i>.
<br><br>
	When <i>codec</i> is called,
	all parameters required by <i>codec</i> is contained in <i>arg</i>.
	<i>src</i> points input buffer block,
	<i>srcfmt</i> contains input encoding parameter,
	<i>dst</i> points output buffer block and
	<i>dstfmt</i> contains output encoding parameter.
	<i>count</i> represents the number of frames to process on this call.
	<i>src</i> and <i>dst</i> are guaranteed to be able to consecutively access
	number of frames specified by <i>count</i>.
	<i>codec</i> must fill entire <i>dst</i>.
	For example, let count = 100, srcfmt is { precision = 16, channels = 3 },
	dstfmt is { precision = 8, channels = 4 } ,
	in this case,
	src block length = 2(bytes) * 3(channels) * 100(frames) = 600 bytes,
	The length to be written to dst block is 1(byte) * 4(channels) * 100(frames)
	= 400 bytes.
	<i>codec</i> cannot abort the conversion halfway and there is no
	error reporting mechanism.
	<i>context</i> is a opaque pointer that can be used by codec if necessary.
<br><br>
	     If the device does not have the <tt>AUDIO_PROP_INDEPENDENT</tt> property
	     the same value is passed in both play and rec.
	     Return 0 on success, otherwise an error code.
</span>
	<span class=a2>It is called at Attach or Closed phase.</span>
</dd>

<dt class=p1>
     int round_blocksize(void *hdl, int bs, int mode,
	     const audio_params_t *param)
</dt>
<dd class=p1>
	     optional, is called with the block size, bs, that has been
	     computed by the upper layer, mode, <tt>AUMODE_PLAY</tt> or <tt>AUMODE_RECORD</tt>,
	     and param, encoding parameters for the hardware.  It should
	     return a block size, possibly changed according to the needs of
	     the hardware driver.
		<span class=a2>It is called at Attach or Closed phase.</span>
<br>
<span class=comment>
いくつかのドライバでは必要もないのに与えられた blocksize の変更を指示するものが
ある。これらは実際 N8 までは問題はなかったが、AUDIO2 では問題となる。
特に多いのが以下の2つの誤解(と思われるもの)。
1つ目は、指示された値を 2^n に切り上げるか切り捨てるなどしたほうが
アラインメントの観点から幸せだろうというコード。
呼び出し側(audio layer) には round_blocksize() の戻り値の下位何ビットが
ゼロだったらどうこうという処理は
最初から書いてないのでこれは(少なくとも現在ではたぶん)何の意味もない。
2つ目は OSS Audio がブロックサイズは 2^n であることを要求しているという誤解。
これそのものが誤解である上に、さらにそもそもここはレイヤーが違う。
この2つの理由以外で、
指定の blocksize が設定できないハードウェアがあれば、
現在の AUDIO2 は対応していないので、その場合は要検討。
</span>
</dd>

<dt class=p1>
     int commit_settings(void *hdl)
</dt>
<dd class=p1>
	     optional, is called after all calls to set_params, and set_port,
	     are done.	A hardware driver that needs to get the hardware in
	     and out of command mode for each change can save all the changes
	     during previous calls and do them all here.  Return 0 on success,
	     otherwise an error code.
		<span class=a2>It is called at Attach or Closed phase.</span>
</dd>

<dt class=p1>
     int init_output(void *hdl, void *buffer, int size)
</dt>
<dd class=p1>
	     optional, is called before any output starts, but when the total
	     size of the output buffer has been determined.  It can be used to
	     initialize looping DMA for hardware that needs that.  Return 0 on
	     success, otherwise an error code.
		<span class=a2>It is called at Attach or Closed phase.</span>
</dd>

<dt class=p1>
     int init_input(void *hdl, void *buffer, int size)
</dt>
<dd class=p1>
	     optional, is called before any input starts, but when the total
	     size of the input buffer has been determined.  It can be used to
	     initialize looping DMA for hardware that needs that.  Return 0 on
	     success, otherwise an error code.
		<span class=a2>It is called at Attach or Closed phase.</span>
</dd>

<dt class=p1>
     int start_output(void *hdl, void *block, int blksize,
	     void (*intr)(void*), void *intrarg)
</dt>
<dd class=p1>
	     is called to start the transfer of blksize bytes from block to
	     the audio hardware.  The call should return when the data
	     transfer has been initiated (normally with DMA).  When the
	     hardware is ready to accept more samples the function intr should
	     be called with the argument intrarg.  Calling intr will normally
	     initiate another call to start_output.  Return 0 on success,
	     otherwise an error code.
		<span class=a2>It is called at Opened phase.</span>
</dd>

<dt class=p1>
     int start_input(void *hdl, void *block, int blksize,
	     void (*intr)(void*), void *intrarg)
</dt>
<dd class=p1>
	     is called to start the transfer of blksize bytes to block from
	     the audio hardware.  The call should return when the data
	     transfer has been initiated (normally with DMA).  When the
	     hardware is ready to deliver more samples the function intr
	     should be called with the argument intrarg.  Calling intr will
	     normally initiate another call to start_input.  Return 0 on
	     success, otherwise an error code.
		<span class=a2>It is called at Opened phase.</span>
</dd>

<dt class=p1>
     int halt_output(void *hdl)
</dt>
<dd class=p1>
	     is called to abort the output transfer (started by start_output)
	     in progress.  Return 0 on success, otherwise an error code.
		<span class=a2>It is called at Opened phase.</span>
</dd>

<dt class=p1>
     int halt_input(void *hdl)
</dt>
<dd class=p1>
	     is called to abort the input transfer (started by start_input) in
	     progress.	Return 0 on success, otherwise an error code.
		<span class=a2>It is called at Opened phase.</span>
</dd>

<dt class=p1>
     int speaker_ctl(void *hdl, int on)
</dt>
<dd class=p1>
	     optional, is called when a half duplex device changes between
	     playing and recording.  It can, e.g., be used to turn on and off
	     the speaker.  Return 0 on success, otherwise an error code.
		<span class=a2>It is called at Opened phase.</span>
</dd>

<dt class=p1>
     int getdev(void *hdl, struct audio_device *ret)
</dt>
<dd class=p1>
	     Should fill the audio_device struct with relevant information
	     about the driver.	Return 0 on success, otherwise an error code.
		<span class=a2>It is called at Opened phase.</span>
</dd>

<dt class=p1><span class=del>
     int setfd(void *hdl, int fd)
</span></dt>
<dd class=p1><span class=del>
	     optional, is called when <tt>AUDIO_SETFD</tt> is used, but only if the
	     device has <tt>AUDIO_PROP_FULLDUPLEX</tt> set.  Return 0 on success,
	     otherwise an error code.
</span></dd>
<dd class=p1><span class=comment>
No drivers implement this function.
</span></dd>

<dt class=p1>
     int set_port(void *hdl, mixer_ctrl_t *mc)
</dt>
<dd class=p1>
	     is called in when <tt>AUDIO_MIXER_WRITE</tt> is used.  It should take data
	     from the mixer_ctrl_t struct at set the corresponding mixer
	     values.  Return 0 on success, otherwise an error code.
		<span class=a2>It is called at Opened or Closed phase.</span>
</dd>

<dt class=p1>
     int get_port(void *hdl, mixer_ctrl_t *mc)
</dt>
<dd class=p1>
	     is called in when <tt>AUDIO_MIXER_READ</tt> is used.  It should fill the
	     mixer_ctrl_t struct.  Return 0 on success, otherwise an error
	     code.
		<span class=a2>It is called at Opened or Closed phase.</span>
</dd>

<dt class=p1>
     int query_devinfo(void *hdl, mixer_devinfo_t *di)
</dt>
<dd class=p1>
	     is called in when <tt>AUDIO_MIXER_DEVINFO</tt> is used.  It should fill
	     the mixer_devinfo_t struct.  Return 0 on success, otherwise an
	     error code.
		<span class=a2>It is called at any time.</span>
</dd>

<dt class=p1>
     void *allocm(void *hdl, int direction, size_t size);
</dt>
<dd class=p1>
	     optional, is called to allocate the device buffers.  If not
	     present malloc(9) is used instead (with the same arguments but
	     the first two).  The reason for using a device dependent routine
	     instead of malloc(9) is that some buses need special allocation
	     to do DMA.	 Returns the address of the buffer, or NULL on failure.
		<span class=a2>It is called at Attached or Closed phase.</span>
</dd>

<dt class=p1>
     void freem(void *hdl, void *addr, size_t size);
</dt>
<dd class=p1>
	     optional, is called to free memory allocated by allocm.  If not
	     supplied free(9) is used.
		<span class=a2>It is called at Attached or Closed phase.</span>
</dd>

<dt class=p1>
     size_t round_buffersize(void *hdl, int direction, size_t bufsize)
</dt>
<dd class=p1>
	     optional, is called at startup to determine the audio buffer
	     size.  The upper layer supplies the suggested size in bufsize,
	     which the hardware driver can then change if needed.  E.g., DMA
	     on the ISA bus cannot exceed 65536 bytes.
		<span class=a2>It is called at Attached or Closed phase.</span>
<br>
<span class=comment>
これも round_blocksize 同様、
無意味に指定値を変更するドライバは AUDIO2 で問題となるかもしれない。
round_blocksize() のコメント参照。
ただしこちらは blocksize とは違って (DMA の制約のように)
実際に設定できない場合があることが分かっているので、
それに該当してしまったらどうすればいいかという考察はまだしていない。
</span>
</dd>

<dt class=p1><span class=del>
     paddr_t mappage(void *hdl, void *addr, off_t offs, int prot)
</span></dt>
<dd class=p1><span class=del>
	     optional, is called for mmap(2).  Should return the map value for
	     the page at offset offs from address addr mapped with protection
	     prot.  Returns -1 on failure, or a machine dependent opaque value
	     on success.
</span></dd>
<dd class=p1><span class=comment>
Now mmap is done by audio layer.
</span></dd>

<dt class=p1>
     int get_props(void *hdl)
</dt>
<dd class=p1>
	     Should return the device properties; i.e., a combination of
	     <tt>AUDIO_PROP_xxx</tt>.
		<span class=a2>
		It is called at any time.
		<br><br>
		<tt>AUDIO_PROP_MMAP</tt> is acceptable but obsoleted, so
		new drivers should not return this property.
		</span>
</dd>

<dt class=p1>
     int trigger_output(void *hdl, void *start, void *end,
	     int blksize, void (*intr)(void*), void *intrarg,
	     const audio_params_t *param)
</dt>
<dd class=p1>
	     optional, is called to start the transfer of data from the
	     circular buffer delimited by start and end to the audio hardware,
	     parameterized as in param.	 The call should return when the data
	     transfer has been initiated (normally with DMA).  When the
	     hardware is finished transferring each blksize sized block, the
	     function intr should be called with the argument intrarg
	     (typically from the audio hardware interrupt service routine).
	     Once started the transfer may be stopped using halt_output.
	     Return 0 on success, otherwise an error code.
		<span class=a2>It is called at Opened phase.</span>
</dd>

<dt class=p1>
     int trigger_input(void *hdl, void *start, void *end,
	     int blksize, void (*intr)(void*), void *intrarg,
	     const audio_params_t *param)
</dt>
<dd class=p1>
	     optional, is called to start the transfer of data from the audio
	     hardware, parameterized as in param, to the circular buffer
	     delimited by start and end.  The call should return when the data
	     transfer has been initiated (normally with DMA).  When the
	     hardware is finished transferring each blksize sized block, the
	     function intr should be called with the argument intrarg
	     (typically from the audio hardware interrupt service routine).
	     Once started the transfer may be stopped using halt_input.
	     Return 0 on success, otherwise an error code.
		<span class=a2>It is called at Opened phase.</span>
</dd>

<dt class=p1>
     int dev_ioctl(void *hdl, u_long cmd, void *addr,
	     int flag, struct lwp *l)
</dt>
<dd class=p1>
	     optional, is called when an ioctl(2) is not recognized by the
	     generic audio driver.  Return 0 on success, otherwise an error
	     code.
		<span class=a2>It is called at Opened phase.</span>
</dd>

<dt class=p1>
     void get_locks(void *hdl, kmutex_t **intr, kmutex_t **thread)
</dt>
<dd class=p1>
	     Returns the interrupt and thread locks to the common audio layer.
		<span class=a2>It is called at Attach phase.</span>
</dd>

</dl>

<p class=p1><span class=comment>
N7、N8 については仕様というより確認できた限りというだけ。
AUDIO2 は仕様。
表中 AUDIO2 列の青字は NetBSD7 と異なる箇所。
仕様は本文中に書いたので
この表はメンテしやすさのために残してあるだけ。
</span></p>
<table border=1 rules=all class=p1 style="margin-left: 12%">
<tr><th>hw_if<th>NetBSD&lt;=7<th>NetBSD&gt;=8<th>AUDIO2
<tr><td>open
 <td>Closed
 <td>たぶんClosed
 <td>Closed

<tr><td>close
 <td>Opened
 <td>たぶんOpened
 <td>Opened

<tr><td>drain
 <td>Opened
 <td>Opened
 <td><span class=a2>(削除)</span>

<tr><td>query_encoding
 <td>Opened
 <td><span class=bad>Never called</span>
 <td><span class=a2>(削除)</span>

<tr><td>set_params
 <td>Opened
 <td>AttachとたぶんClosed
 <td><span class=a2>(将来廃止予定)<br>
	Attach, Closed
	</span>

<tr><td>round_blocksize
 <td>Opened
 <td>Attach。他は不明。
 <td><span class=a2>Attach, Closed</span>

<tr><td>commit_settings
 <td>Opened
 <td>Attach。他は不明。
 <td><span class=a2>Attach, Closed</span>

<tr><td>init_output<br>init_input
 <td>Opened
 <td><span class=bad>AttachとたぶんClosed</span>
 <td><span class=a2>Attach, Closed</span>

<tr><td>start_output<br>start_input
 <td>Opened
 <td>たぶんOpened
 <td>Opened

<tr><td>halt_output<br>halt_input
 <td>Opened
 <td>たぶんOpened
 <td>Opened

<tr><td>speaker_ctl
 <td>Opened
 <td>たぶんOpened
 <td>Opened

<tr><td>getdev
 <td>Opened
 <td>Opened
 <td>Opened

<tr><td>setfd
 <td>Opened
 <td><span class=bad>Opened</span>
 <td><span class=a2>(削除)</span>

<tr><td>set_port<br>get_port
 <td colspan=3>未調査

<tr><td>query_devinfo
 <td colspan=3>at all time ? (未調査)

<tr><td>allocm<br>freem
 <td>Attach
 <td>AttachとたぶんClosed
 <td><span class=a2>Attach, Closed</span>

<tr><td>round_buffersize
 <td>Attach
 <td>AttachとたぶんClosed
 <td><span class=a2>Attach, Closed</span>

<tr><td>mappage
 <td>Opened
 <td><span class=bad>Never called</span>
 <td><span class=a2>(削除)</span>

<tr><td>get_props
 <td>AttachとOpened
 <td>Attachとたぶんattach以降
 <td><span class=a2>at all time</span>

<tr><td>trigger_output<br>trigger_input
 <td>Opened
 <td>たぶんOpened
 <td>Opened

<tr><td>dev_ioctl
 <td>Opened
 <td>Opened
 <td>Opened

<tr><td>get_locks
 <td>Attach
 <td>Attach
 <td>Attach

<tr><td>query_format
 <td>&mdash;
 <td>&mdash;
 <td><span class=a2>(新設)<br>at all time</span>

<tr><td>set_format
 <td>&mdash;
 <td>&mdash;
 <td><span class=a2>(新設)<br>
	Attach, Closed</span>

</table>

<p class=p1>
     The query_devinfo method should define certain mixer controls for
     <tt>AUDIO_SETINFO</tt> to be able to change the port and gain, and
	<tt>AUDIO_GETINFO</tt>
     to read them, as follows.
</p>

<p class=p1>
     If the record mixer is capable of input from more than one source, it
     should define AudioNsource in class AudioCrecord.	This mixer control
     should be of type AUDIO_MIXER_ENUM or AUDIO_MIXER_SET and enumerate the
     possible input sources.  Each of the named sources for which the
     recording level can be set should have a control in the AudioCrecord
     class of type AUDIO_MIXER_VALUE, except the "mixerout" source is special,
     and will never have its own control.  Its selection signifies, rather,
     that various sources in class AudioCrecord will be combined and presented
     to the single recording output in the same fashion that the sources of
     class AudioCinputs are combined and presented to the playback output(s).
     If the overall recording level can be changed, regardless of the input
     source, then this control should be named AudioNmaster and be of class
     AudioCrecord.
</p>

<p class=p1>
     Controls for various sources that affect only the playback output, as
     opposed to recording, should be in the AudioCinputs class, as of course
     should any controls that affect both playback and recording.
</p>

<p class=p1>
     If the play mixer is capable of output to more than one destination, it
     should define AudioNselect in class AudioCoutputs.	 This mixer control
     should be of type AUDIO_MIXER_ENUM or AUDIO_MIXER_SET and enumerate the
     possible destinations.  For each of the named destinations for which the
     output level can be set, there should be a control in the AudioCoutputs
     class of type AUDIO_MIXER_VALUE.  If the overall output level can be
     changed, which is invariably the case, then this control should be named
     AudioNmaster and be of class AudioCoutputs.
</p>

<p class=p1>
     There's one additional source recognized specially by AUDIO_SETINFO and
     AUDIO_GETINFO, to be presented as monitor_gain, and that is a control
     named AudioNmonitor, of class AudioCmonitor.
</p>


<b>SEE ALSO</b>
<p class=p1>
     audio(4)
</p>

<b>HISTORY</b>
<p class=p1>
     This audio interface first appeared in NetBSD 1.3.
</p>


<hr>
</body>
</html>
